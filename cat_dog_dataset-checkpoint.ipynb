{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n",
      "total training dog images: 1000\n",
      "total validation cat images: 500\n",
      "total validation dog images: 500\n",
      "total test cat images: 500\n",
      "total test dog images: 500\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n",
      "[0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 168s 2s/step - loss: 0.6885 - acc: 0.5445 - val_loss: 0.7565 - val_acc: 0.5020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 269s 3s/step - loss: 0.6641 - acc: 0.5945 - val_loss: 0.6526 - val_acc: 0.5430\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 259s 3s/step - loss: 0.6256 - acc: 0.6630 - val_loss: 0.5488 - val_acc: 0.6520\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 460s 5s/step - loss: 0.5822 - acc: 0.7010 - val_loss: 0.7819 - val_acc: 0.6450\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.5547 - acc: 0.7165 - val_loss: 0.5757 - val_acc: 0.6860\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 499s 5s/step - loss: 0.5284 - acc: 0.7375 - val_loss: 0.5362 - val_acc: 0.6730\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 259s 3s/step - loss: 0.4978 - acc: 0.7640 - val_loss: 0.4877 - val_acc: 0.7000\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 377s 4s/step - loss: 0.4758 - acc: 0.7670 - val_loss: 0.5873 - val_acc: 0.7090\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 424s 4s/step - loss: 0.4485 - acc: 0.7965 - val_loss: 0.4883 - val_acc: 0.6960\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 475s 5s/step - loss: 0.4238 - acc: 0.7980 - val_loss: 0.5195 - val_acc: 0.7210\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 305s 3s/step - loss: 0.4033 - acc: 0.8100 - val_loss: 0.5298 - val_acc: 0.7260\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 372s 4s/step - loss: 0.3690 - acc: 0.8400 - val_loss: 0.5712 - val_acc: 0.7230\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 283s 3s/step - loss: 0.3445 - acc: 0.8535 - val_loss: 0.4322 - val_acc: 0.7280\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 544s 5s/step - loss: 0.3168 - acc: 0.8605 - val_loss: 0.7866 - val_acc: 0.7270\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.2995 - acc: 0.8810 - val_loss: 0.3727 - val_acc: 0.7330\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 650s 7s/step - loss: 0.2725 - acc: 0.8935 - val_loss: 0.3407 - val_acc: 0.7140\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 219s 2s/step - loss: 0.2539 - acc: 0.9030 - val_loss: 0.4781 - val_acc: 0.7380\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 381s 4s/step - loss: 0.2402 - acc: 0.9015 - val_loss: 0.7023 - val_acc: 0.7040\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 243s 2s/step - loss: 0.2104 - acc: 0.9235 - val_loss: 0.3945 - val_acc: 0.7300\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 347s 3s/step - loss: 0.1869 - acc: 0.9310 - val_loss: 0.7635 - val_acc: 0.7140\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 231s 2s/step - loss: 0.1656 - acc: 0.9455 - val_loss: 0.5644 - val_acc: 0.7340\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 260s 3s/step - loss: 0.1490 - acc: 0.9410 - val_loss: 1.1528 - val_acc: 0.7270\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 326s 3s/step - loss: 0.1286 - acc: 0.9590 - val_loss: 0.2229 - val_acc: 0.7110\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 222s 2s/step - loss: 0.1130 - acc: 0.9620 - val_loss: 0.3625 - val_acc: 0.7350\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 341s 3s/step - loss: 0.0979 - acc: 0.9665 - val_loss: 1.0723 - val_acc: 0.7250\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 250s 3s/step - loss: 0.0897 - acc: 0.9730 - val_loss: 0.7610 - val_acc: 0.7290\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 5238s 52s/step - loss: 0.0743 - acc: 0.9795 - val_loss: 0.6052 - val_acc: 0.7210\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 133s 1s/step - loss: 0.0565 - acc: 0.9860 - val_loss: 0.4278 - val_acc: 0.6950\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0525 - acc: 0.9855 - val_loss: 2.4680 - val_acc: 0.7220\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 146s 1s/step - loss: 0.0459 - acc: 0.9885 - val_loss: 0.6960 - val_acc: 0.7130\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import os, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "original_dataset_dir = 'F:/catsanddogs/train'\n",
    "base_dir = 'F:/PIAIC/new_dataset'\n",
    "os.mkdir(base_dir)\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "#Directories for the training,validation, andtest splits\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "#Directory with training cat pictures\n",
    "\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "#Directory with training dog pictures\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "#Directory with validation cat pictures\n",
    "\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "#Directory with validation dog pictures\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "#Directory with test cat pictures\n",
    "\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src= os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#Copies the first 1,000 cat images to train_cats_dir\n",
    "\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#Copies the next 500 cat images tovalidation_cats_dir\n",
    "\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#Copies the next 500 cat images to test_cats_dir\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#Copies the first 1,000 dog images to train_dogs_dir\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#Copies the next 500\n",
    "#dog images to\n",
    "#validation_dogs_dir\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "from keras import optimizers\n",
    "model.compile(loss='binary_crossentropy',\n",
    "optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "metrics=['acc'])\n",
    "\n",
    "import tensorflow\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,target_size=(150, 150),batch_size=20,class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,target_size=(150, 150),batch_size=20,class_mode='binary')\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    plt.imshow(data_batch[0])\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    print(labels_batch)\n",
    "    break\n",
    "#Fitting the model using a batch generator\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=30,validation_data=validation_generator,validation_steps=50)\n",
    "\n",
    "\n",
    "model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
